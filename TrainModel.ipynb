{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "153hiwpSUnm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from urllib.request import urlopen\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLyA1Rv3UuJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "c8e7735c-d782-49be-d64d-1c26fbf12623"
      },
      "source": [
        "url = 'https://www.gutenberg.org/files/174/174.txt'\n",
        "text = urlopen(url).read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(50)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 462103 characters\n",
            "85 unique characters\n",
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '#' :   5,\n",
            "  '$' :   6,\n",
            "  '%' :   7,\n",
            "  \"'\" :   8,\n",
            "  '(' :   9,\n",
            "  ')' :  10,\n",
            "  '*' :  11,\n",
            "  ',' :  12,\n",
            "  '-' :  13,\n",
            "  '.' :  14,\n",
            "  '/' :  15,\n",
            "  '0' :  16,\n",
            "  '1' :  17,\n",
            "  '2' :  18,\n",
            "  '3' :  19,\n",
            "  '4' :  20,\n",
            "  '5' :  21,\n",
            "  '6' :  22,\n",
            "  '7' :  23,\n",
            "  '8' :  24,\n",
            "  '9' :  25,\n",
            "  ':' :  26,\n",
            "  ';' :  27,\n",
            "  '?' :  28,\n",
            "  '@' :  29,\n",
            "  'A' :  30,\n",
            "  'B' :  31,\n",
            "  'C' :  32,\n",
            "  'D' :  33,\n",
            "  'E' :  34,\n",
            "  'F' :  35,\n",
            "  'G' :  36,\n",
            "  'H' :  37,\n",
            "  'I' :  38,\n",
            "  'J' :  39,\n",
            "  'K' :  40,\n",
            "  'L' :  41,\n",
            "  'M' :  42,\n",
            "  'N' :  43,\n",
            "  'O' :  44,\n",
            "  'P' :  45,\n",
            "  'Q' :  46,\n",
            "  'R' :  47,\n",
            "  'S' :  48,\n",
            "  'T' :  49,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GouogB2cUubb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "a716f5a7-5260-4ace-9517-a67f949665ae"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "P\n",
            "'The Project Gutenberg EBook of The Picture of Dorian Gray, by Oscar Wilde\\r\\n\\r\\nThis eBook is for the us'\n",
            "'e of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give i'\n",
            "'t away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or on'\n",
            "'line at www.gutenberg.net\\r\\n\\r\\n\\r\\nTitle: The Picture of Dorian Gray\\r\\n\\r\\nAuthor: Oscar Wilde\\r\\n\\r\\nRelease Da'\n",
            "'te: June 9, 2008 [EBook #174]\\r\\n[This file last updated on July 2, 2011]\\r\\n[This file last updated on J'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9PW7TLqUuuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13aVScAGUunn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGAUIwefUuiy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37d0728b-7bf9-48cf-8336-e0985af3b15d"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DQtqgRBUufu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWIADy26w_mV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kpMyG2Nw_wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEhpXgluxryK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0555551-dc95-40ef-bacb-ad19ec0ee778"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 85) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uTxygBvxr5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "09a86b59-9cac-4f30-e626-618de2c85c0a"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'handed to his master.\\r\\n\"Her Grace told me to wait for an answer,\" he murmured.\\r\\n\\r\\nDorian put the let'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'Xg:e:#zn;2?Q\"smb\\r2@G\\re5rgVPVNOX]093 -N_B)s_YP;.a64!%)tTtV:fBYw4?F\\'k?nnHxE)z7,:xumO1/@P/srEgoUZfxebn@'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OX3wCVPxsAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3295cebc-4a07-45b3-dd40-fa26b9f298bf"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 85)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4442515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pEDUWHIxsGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB2baYAXw_zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUPmD1cEw_2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b878445-5bfe-4643-aa01-003e97be69c1"
      },
      "source": [
        "EPOCHS=35\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 3.1583\n",
            "Epoch 2/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 2.2881\n",
            "Epoch 3/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 2.1007\n",
            "Epoch 4/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.9211\n",
            "Epoch 5/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.7729\n",
            "Epoch 6/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.6453\n",
            "Epoch 7/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.5414\n",
            "Epoch 8/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.4581\n",
            "Epoch 9/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.3905\n",
            "Epoch 10/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.3342\n",
            "Epoch 11/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.2836\n",
            "Epoch 12/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.2385\n",
            "Epoch 13/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.1958\n",
            "Epoch 14/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.1543\n",
            "Epoch 15/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.1140\n",
            "Epoch 16/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.0730\n",
            "Epoch 17/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 1.0320\n",
            "Epoch 18/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.9883\n",
            "Epoch 19/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.9462\n",
            "Epoch 20/35\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.9047\n",
            "Epoch 21/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.8598\n",
            "Epoch 22/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.8131\n",
            "Epoch 23/35\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.7668\n",
            "Epoch 24/35\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.7242\n",
            "Epoch 25/35\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.6792\n",
            "Epoch 26/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.6384\n",
            "Epoch 27/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.6000\n",
            "Epoch 28/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5639\n",
            "Epoch 29/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5303\n",
            "Epoch 30/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4989\n",
            "Epoch 31/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4699\n",
            "Epoch 32/35\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4484\n",
            "Epoch 33/35\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4284\n",
            "Epoch 34/35\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4104\n",
            "Epoch 35/35\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.3935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXJYSMOzw_5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "dba174ff-9cb8-4445-96b9-08d67f9349bc"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_35'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAxl2FSX2WZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCRuAoIN2Whc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e404fbc5-0e83-4e0b-f16e-913542082885"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            21760     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 85)             87125     \n",
            "=================================================================\n",
            "Total params: 4,047,189\n",
            "Trainable params: 4,047,189\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCNMdIU_2n7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u20WuVN42n-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "601aa13f-7384-4246-a52d-5fcca6a705ad"
      },
      "source": [
        "print(generate_text(model, start_string=u\"For\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ford Henry, like\r\n",
            "the day.  The reason who it was to\r\n",
            "tea-table.  Herever e missed\r\n",
            "by his sicts experience.\r\n",
            "He had marred him.\r\n",
            "\r\n",
            "Turned his spoke and stood looking of a coupt that had mould have to speak to him, some nd my whole question.\"\r\n",
            "\r\n",
            "The elder woman who I am\r\n",
            "to forget you, Harry.  It is not my words:\r\n",
            "Shry afraid of himself.  But the world might go torrs.  Royal, he flum his hat and cape on the table, and Pattis work at dinner\r\n",
            "name and in front of the\r\n",
            "purpher and satisfife.  What is the number of your life.  Keep your horrible lend.  I met his a meat-seathe one cheeks, and anyoked about the sentence,\r\n",
            "sometimes with a deep note of a certain\r\n",
            "Vone.  It would be a real playinger with red gulters\r\n",
            "became diarary of chiefly of pleasures, or alone with the top of the guns?  He bettered now.  A\r\n",
            "tridus villages that seemed to end his\r\n",
            "dreadful, frong the cabinet woman's\r\n",
            "lucky from paradoxes, and that\r\n",
            "\r\n",
            "Did it mnem like their secret.  When they met and watched the flaring stips.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAlkarbD2oCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "760e4b5c-b45e-45ab-c5a3-f0aa451bdefb"
      },
      "source": [
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly2S4HB92oFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsfUVqIr2oJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}